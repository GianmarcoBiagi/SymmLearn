var documenterSearchIndex = {"docs":
[{"location":"api/#SymmLearn.jl-API","page":"API","title":"SymmLearn.jl API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"This page lists all user-facing functions in SymmLearn.jl.   These are the primary entry points you will use to process data, build models, train them, and make predictions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"SymmLearn.xyz_to_nn_input\nSymmLearn.build_species_models\nSymmLearn.train_model!\nSymmLearn.dispatch\nSymmLearn.predict_forces\nSymmLearn.loss","category":"page"},{"location":"api/#SymmLearn.xyz_to_nn_input","page":"API","title":"SymmLearn.xyz_to_nn_input","text":"xyz_to_nn_input(file_path::String)\n\nProcess an XYZ file containing atomic structures and energies to generate datasets for neural network training.\n\nArguments\n\nfile_path::String: Path to the XYZ file containing coordinates, species, lattice cells, and energy values.\n\nReturns\n\nA tuple with:\n\nx_train::Array, y_train::Vector{Sample}: Training inputs and targets.\nx_val::Array, y_val::Vector{Sample}: Validation inputs and targets.\nx_test::Array, y_test::Vector{Sample}: Test inputs and targets.\n(energy_mean::Float32, energy_std::Float32): Energy normalization statistics.\nunique_species::Vector{String}: Unique atomic species in the dataset.\nspecies_idx::Dict{String,Int}: Mapping from species to integer indices.\nall_cells::Vector{Matrix{Float32}}: Lattice cell matrices for each configuration.\n\nDescription\n\nSteps performed:\n\nExtract structures, species, cells, and energies with extract_data.\nConvert positions and forces into atom-wise inputs with prepare_nn_data.\nNormalize energies and rescale forces consistently, then split into train/val/test sets with data_preprocess.\n\nDependencies\n\nextract_data(file_path): Parses raw atomic data from the XYZ file.  \nprepare_nn_data(dataset, species, unique_species): Builds NN inputs and forces arrays.  \ndata_preprocess(nn_input_dataset, all_energies, all_forces): Normalizes and splits the dataset.\n\nExample\n\nfile_path = \"example_structures.xyz\"\nx_train, y_train, x_val, y_val, x_test, y_test, (energy_mean, energy_std), unique_species, species_idx, all_cells = xyz_to_nn_input(file_path)\nprintln(\"Training set size: \", length(x_train))\nprintln(\"Unique species: \", unique_species)\n\n\n\n\n\n","category":"function"},{"location":"api/#SymmLearn.build_species_models","page":"API","title":"SymmLearn.build_species_models","text":"build_species_models(unique_species::Vector{String}, species_idx::Dict{String,Int}, G1_number::Int, R_cutoff::Float32)\n\nCreates an array of Flux Chain models, one for each unique species.   The array is indexed according to species_idx, so that each species can be dispatched to the correct model based on its numeric index.  \n\nArguments\n\nunique_species::Vector{String}: List of species names.\nspecies_idx::Dict{String,Int}: Mapping from species name to numeric index.\nG1_number::Int: Number of neurons in the G1Layer.\nR_cutoff::Float32: Cutoff radius for the G1Layer.\n\nReturns\n\nspecies_models::Vector{Chain}: Array of models, ready for Enzyme differentiation.\n\nunique_species = [\"H\", \"O\"]\nspecies_idx = Dict(\"H\"=>1, \"O\"=>2)\nG1_number = 5\nR_cutoff = 5.0f0\n\nmodels = build_species_models(unique_species, species_idx, G1_number, R_cutoff; depth=2)\nprintln(models[1])  # model for H\nprintln(models[2])  # model for O\n\n\n\n\n\n","category":"function"},{"location":"api/#SymmLearn.train_model!","page":"API","title":"SymmLearn.train_model!","text":"train_model!(model, x_train, y_train, x_val, y_val, \n             λ=1.0f0; forces=true, initial_lr=0.01, min_lr=1e-6, decay_factor=0.1, \n             patience=50, epochs=1000, batch_size=32, verbose=false, lattice=nothing)\n\nTrain a neural network model to predict total energies (and optionally forces)  for atomic structures using mini-batch gradient descent, adaptive learning rate,  and early stopping.\n\nArguments\n\nmodel::Flux.Chain   Neural network to train. Can combine multiple branches for different atomic species.\nx_train::Any   Training atomic structures, either batched or compatible with distance_layer.\ny_train::Vector{Sample}   Ground-truth labels containing .energy and .forces for training.\nx_val::Any   Validation atomic structures, same format as x_train.\ny_val::Vector{Sample}   Ground-truth labels for validation.\nλ::Float32 (default = 1.0)   Weight applied to the force loss relative to the energy loss.\nforces::Bool (default = true)   If true, include force loss in addition to energy loss.\ninitial_lr::Float32 (default = 0.01)   Initial learning rate for the Adam optimizer.\nmin_lr::Float32 (default = 1e-6)   Minimum allowed learning rate; training stops decaying when reached.\ndecay_factor::Float32 (default = 0.1)   Factor by which to multiply the learning rate if validation loss plateaus.\npatience::Int (default = 50)   Number of epochs without improvement before reducing the learning rate.\nepochs::Int (default = 1000)   Maximum number of training epochs.\nbatch_size::Int (default = 32)   Number of structures per mini-batch.\nverbose::Bool (default = false)   If true, prints progress, learning rate changes, and final results.\nlattice::Union{Nothing, Matrix{Float32}} (default = nothing)   Optional 3×3 lattice matrix if distances should be computed under PBC.\n\nReturns\n\nbest_model::Flux.Chain   Model achieving the lowest validation loss during training.\nloss_tr::Vector{Float32}   Training loss per epoch.\nloss_val::Vector{Float32}   Validation loss per epoch.\n\nDescription\n\nPrecomputes pairwise distances (distance_layer) and derivatives (distance_derivatives) for both training and validation sets.\nPerforms mini-batch gradient descent using Enzyme.gradient and Flux.update!.\nOptionally computes forces in addition to energies in the loss function.\nApplies adaptive learning rate: decays learning rate if validation loss does not improve for patience epochs.\nSaves the model achieving the lowest validation loss (best_model).\nSupports early stopping if the learning rate reaches min_lr.\n\nNotes\n\nForces are computed using analytical derivatives of distances and backpropagated through the model.\nThe λ parameter balances energy and force contributions in the total loss.\nLoss values are monitored to prevent NaNs; training will stop if a NaN is detected.\n\n```julia\n\nAssume models, xtrain, ytrain, xval, yval are defined\n\nbestmodel, trainloss, valloss = trainmodel!(     model, xtrain, ytrain, xval, yval;     λ=0.5f0, forces=true, initiallr=0.01, epochs=500, batchsize=16, verbose=true )\n\nprintln(\"Training finished. Best validation loss: \", minimum(val_loss))```\n\n\n\n\n\n","category":"function"},{"location":"api/#SymmLearn.dispatch","page":"API","title":"SymmLearn.dispatch","text":"dispatch(atoms, species_models::Vector{Chain}; lattice::Union{Nothing, Matrix{Float32}} = nothing)\n\nPublic API function.  The developer version is called dispatchtrain.  Computes the distance representation of a set of atoms (optionally within a lattice),   then applies the appropriate species-specific models via `dispatchtrain`.  \n\nArguments\n\natoms: Atomic structure input, suitable for distance_layer.  \nspecies_models::Vector{Chain}: One neural network model per species.  \nlattice::Union{Nothing, Matrix{Float32}}: Optional lattice matrix for periodic systems.    Defaults to nothing (no periodic boundary conditions).\n\nReturns\n\noutputs: Model predictions, either a scalar (single batch)    or a vector (batched input), depending on the input format.\n\n```julia\n\nSingle configuration\n\natoms = sampleatoms()  # returns a Vector{AtomInput} models = [modelH, model_O]  # one Flux.Chain per species result = dispatch(atoms, models)\n\nBatched configurations\n\nbatchatoms = [atoms1, atoms2, atoms3] batchresult = dispatch(batchatoms, models) println(batchresult)```\n\n\n\n\n\n","category":"function"},{"location":"api/#SymmLearn.predict_forces","page":"API","title":"SymmLearn.predict_forces","text":"predict_forces(x, model; flat=false) -> Array{Float32}\n\nCompute predicted atomic forces for a batch of structures using a trained model.\n\nArguments\n\nx: Input atomic structures, either a Matrix{Vector{AtomInput}} (batched) or compatible with distance_layer.\nmodel: Species-specific neural network models used for force prediction.\nflat::Bool (optional): If true, return forces flattened as a 1D vector;   if false (default), return a 3D array (n_batches, n_atoms, 3).\n\nReturns\n\nArray{Float32}: Predicted forces for all atoms:\n(n_batches, n_atoms, 3) if flat=false\nFlattened 1D array if flat=true\n\nDescription\n\nCompute pairwise distances with distance_layer.\nCompute distance derivatives with distance_derivatives.\nFor each batch, compute force contributions from model gradients.\nOptionally flatten the resulting force array.\n\n```julia\n\nAssume x_test contains a batch of structures and models is defined\n\nforces = predictforces(xtest, models) println(size(forces))  # (nbatches, natoms, 3)\n\nforcesflat = predictforces(xtest, models; flat=true) println(length(forcesflat))  # nbatches * natoms * 3```\n\n\n\n\n\n","category":"function"},{"location":"api/#SymmLearn.loss","page":"API","title":"SymmLearn.loss","text":"loss(model, x, y; λ=1.0f0, forces=true) -> Float32\n\nCompute the training loss as mean squared error on energies plus, optionally, a weighted force-matching term.\n\nThis is a user-level function; the developer version is loss_train.\n\nArguments\n\nmodel::Vector{Chain}: Species-specific neural network models.\nx: Input atomic structure(s).\ny: Reference labels containing energies and forces.\nλ::Float32 (optional): Weight for the force contribution. Default = 1.0.\nforces::Bool (optional): If true, include force loss. If false, only energy loss. Default = true.\n\nReturns\n\nFloat32: Total loss (energy-only if forces=false, energy + λ·forces otherwise).\n\n```julia\n\nAssume models, xbatch, ybatch are defined\n\ntotalloss = loss(models, xbatch, ybatch; λ=0.5f0, forces=true) println(\"Computed loss: \", totalloss)```\n\n\n\n\n\n","category":"function"},{"location":"usage/#Usage","page":"Usage","title":"Usage","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"After installing the package, load it with:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SymmLearn","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"This section illustrates the expected workflow for using SymmLearn.jl, from raw data extraction to model construction and training. The goal is to demonstrate how each core component integrates into a complete machine-learning force field pipeline.","category":"page"},{"location":"usage/#Data-Preparation","page":"Usage","title":"Data Preparation","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"The first step is to extract and preprocess data from an .xyz file containing all configurations. This can be done in a single call:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x_train, y_train, \nx_val,   y_val, \nx_test,  y_test, \ne_mean,  e_std, \nunique_species, species_idx, lattice = \n    xyz_to_nn_input(path_to_your_XYZ_file)\n","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"This function performs all the following:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Parses atomic configurations and extracts energy and force labels\nSplits the dataset into training, validation, and test sets\nNormalizes energies (returns e_mean and e_std for rescaling)\nIdentifies atomic species and their indices\nExtracts the lattice matrix ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Note: Currently, SymmLearn.jl assumes a fixed lattice for all configurations.   The lattice from the first structure is used for the entire dataset.","category":"page"},{"location":"usage/#Model-Construction","page":"Usage","title":"Model Construction","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next, build a model based on the desired level of symmetry and complexity. You can control:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"The number of radial symmetry functions, N_G1\nThe number of angular symmetry functions, N_G2 #TODO\nThe cutoff radius, r_cutoff\nThe network depth, which determines model complexity","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"N_G1 = 5 \nN_G2 = 2\nr_cutoff = 2.75f0\ndepth = 1\n\nmodel = build_species_models(unique_species, species_idx, N_G1, N_G2, r_cutoff , depth = 1 ) ","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Each atomic species gets its own subnetwork. The total energy of the system is computed as the sum of all atomic contributions.","category":"page"},{"location":"usage/#Model-Training","page":"Usage","title":"Model Training","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Once the model is built, train it using the prepared datasets. The training function supports energy-only or energy + force fitting, adaptive learning rate decay, and early stopping. We suggest to check the documentation for the SymmLearn.train_model! function for additional information.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":" trained_model, train_loss, val_loss = train_model!(\n        model,\n        x_train, y_train,\n        x_val, y_val;\n         forces = true,  epochs = 500 , initial_lr = 1e-3 )","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"This process optimizes the neural network parameters to minimize the energy and (optionally) force prediction errors.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now that the machine learning potential has been trained you can check it's loss plot, how well it predicts the data, or even use it for molecular dynamics via the Molly interface!","category":"page"},{"location":"developer/#SymmLearn.jl-Developer-API","page":"SymmLearn.jl Developer API","title":"SymmLearn.jl Developer API","text":"","category":"section"},{"location":"developer/","page":"SymmLearn.jl Developer API","title":"SymmLearn.jl Developer API","text":"This page lists all the internal functions, structs, and utilities of SymmLearn.jl.   These are mainly intended for developers and advanced users who want to understand or extend the library.  ","category":"page"},{"location":"developer/","page":"SymmLearn.jl Developer API","title":"SymmLearn.jl Developer API","text":"The functions here are not included in the main menu, but are fully searchable through the search bar.","category":"page"},{"location":"developer/#SymmLearn.charge_to_element","page":"SymmLearn.jl Developer API","title":"SymmLearn.charge_to_element","text":"charge_to_element :: Dict{Int, String}\n\nInverse mapping of element_to_charge. Maps atomic numbers (charges) to their element symbols.\n\nExample\n\ncharge_to_element[6]  # Returns \"C\"\ncharge_to_element[79] # Returns \"Au\"\n\n\n\n\n\n","category":"constant"},{"location":"developer/#SymmLearn.element_to_charge","page":"SymmLearn.jl Developer API","title":"SymmLearn.element_to_charge","text":"element_to_charge :: Dict{String, Int}\n\nDictionary mapping chemical element symbols (as Strings) to their atomic numbers (as Ints).   This is used to assign a charge or atomic number to an element for neural network input preprocessing.\n\nExample\n\n# Retrieve atomic number of Carbon\ncarbon_atomic_number = element_to_charge[\"C\"]\nprintln(carbon_atomic_number)  # Output: 6\n\n\n\n\n\n","category":"constant"},{"location":"developer/#SymmLearn.AtomInput","page":"SymmLearn.jl Developer API","title":"SymmLearn.AtomInput","text":"AtomInput(species::Int, coord::AbstractVector)\n\nContainer for one atom in a structure.\n\nspecies: integer index in 1..K identifying the species.\ncoord: atomic coordinates as a vector.\n\n\n\n\n\n","category":"type"},{"location":"developer/#SymmLearn.G1Input","page":"SymmLearn.jl Developer API","title":"SymmLearn.G1Input","text":"G1Input(species::Int, dist::AbstractMatrix)\n\nContainer for one atom in a structure.\n\nspecies: integer index in 1..K identifying the species.\ndist: matrix of distances between the atom and the others.\n\n\n\n\n\n","category":"type"},{"location":"developer/#SymmLearn.G1Layer","page":"SymmLearn.jl Developer API","title":"SymmLearn.G1Layer","text":"G1Layer(W_eta::Vector{Float32}, W_Fs::Vector{Float32}, cutoff::Float32, charge::Float32)\n\nCustom neural network layer with weights and system-specific parameters.\n\nFields\n\nW_eta::Vector{Float32}: Weight vector for the \"eta\" connection.\nW_Fs::Vector{Float32}: Weight vector for the \"Fs\" connection.\ncutoff::Float32: Cutoff radius for interactions.\ncharge::Float32: Atomic charge associated with the layer.\n\nExample\n\nW_eta_example = rand(Float32, 5)\nW_Fs_example  = rand(Float32, 5)\ncutoff_example = 5.0f0\ncharge_example = 1.0f0\n\nlayer = G1Layer(W_eta_example, W_Fs_example, cutoff_example, charge_example)\nprintln(\"Layer created: \", layer)\n\n\n\n\n\n","category":"type"},{"location":"developer/#SymmLearn.G1Layer-Tuple{AbstractMatrix{Float32}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.G1Layer","text":"(layer::G1Layer)(x::AbstractMatrix{Float32}) -> Matrix{Float32}\n\nApply the G1Layer to a batch of atomic distances, computing radial symmetry function outputs.\n\nArguments\n\nlayer::G1Layer: Layer instance with fields:\nW_eta::Vector{Float32}: Width parameters for each symmetry function.\nW_Fs::Vector{Float32}: Peak positions for each symmetry function.\ncutoff::Float32: Cutoff radius for neighbor interactions.\ncharge::Float32: Atomic charge scaling factor.\nx::AbstractMatrix{Float32}: Distance matrix of shape (n_batch, n_neighbors).\n\nReturns\n\nMatrix{Float32}: Symmetry function outputs, shape (n_features, n_batch), where n_features = size(layer.W_eta, 1).\n\nExample\n\nlayer = G1Layer(5, 2.5f0, 1.0f0)  # 5 symmetry functions, cutoff 2.5, charge 1.0\nx = rand(Float32, 3, 10)          # 3 atoms, 10 neighbors each\noutput = layer(x)                 # Output shape: (5, 3)\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.G1Layer-Tuple{Int64, Float32, Float32}","page":"SymmLearn.jl Developer API","title":"SymmLearn.G1Layer","text":"G1Layer(N_G1::Int, cutoff::Float32, charge::Float32; seed::Union{Int,Nothing}=nothing) -> G1Layer\n\nCreate a G1 radial symmetry function layer with physically-informed initialization.\n\nFs are distributed linearly across the distance range [r_min, cutoff] with small random jitter.\neta values are set according to the average spacing between Fs, with slight random perturbations.\nThis initialization avoids extreme contributions from very small distances that can bias energy predictions.\n\nArguments\n\nN_G1::Int: Number of G1 functions.\ncutoff::Float32: Cutoff radius for interatomic interactions.\ncharge::Float32: Atomic charge used as scaling factor.\nseed::Int or nothing: Optional RNG seed for reproducibility.\n\nReturns\n\nG1Layer instance with initialized Fs and eta.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.Sample","page":"SymmLearn.jl Developer API","title":"SymmLearn.Sample","text":"Sample(energy::Float32, forces::Array{Float32,2})\n\nContainer for target data of one atomic configuration.\n\nFields\n\nenergy::Float32: Total energy of the configuration.\nforces::Array{Float32,2}: Forces acting on the atoms, shape (num_atoms, 3).\n\n\n\n\n\n","category":"type"},{"location":"developer/#SymmLearn.batch_indices-Tuple{Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.batch_indices","text":"batch_indices(n::Int, batchsize::Int) -> Vector{Vector{Int}}\n\nGenerate mini-batch indices for stochastic gradient descent.\n\nArguments\n\nn::Int: Total number of samples in the dataset.\nbatchsize::Int: Size of each mini-batch.\n\nReturns\n\nVector{Vector{Int}}: Shuffled list of index vectors, each representing a mini-batch.   The last batch may contain fewer than batchsize elements if n is not divisible by batchsize.\n\nNotes\n\nThe indices are shuffled randomly each call to ensure stochasticity.\nUseful for iterating over training data in train_model!.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.build_branch","page":"SymmLearn.jl Developer API","title":"SymmLearn.build_branch","text":"build_branch(atom::String, G1_number::Int, R_cutoff::Float32; depth=2, seed=nothing) -> Chain\n\nConstruct a per-species neural network subbranch for atomic energy prediction.\n\nArguments\n\natom::String: Atomic species name.\nG1_number::Int: Number of radial G1 symmetry functions.\nR_cutoff::Float32: Cutoff radius for the G1Layer.\ndepth::Int (optional): Complexity of hidden layers. 1 or 2. Default = 2.\nseed::Int or nothing (optional): RNG seed for G1Layer initialization.\n\nReturns\n\nChain: Flux.jl neural network chain consisting of:\nG1Layer with scaled atomic charge.\nLayerNorm layers.\nDense layers with swish activation.\nFinal Dense layer outputs scalar atomic energy.\n\n\n\n\n\n","category":"function"},{"location":"developer/#SymmLearn.build_species_models-Tuple{Vector{String}, Dict{String, Int64}, Int64, Float32}","page":"SymmLearn.jl Developer API","title":"SymmLearn.build_species_models","text":"build_species_models(unique_species::Vector{String}, species_idx::Dict{String,Int}, G1_number::Int, R_cutoff::Float32)\n\nCreates an array of Flux Chain models, one for each unique species.   The array is indexed according to species_idx, so that each species can be dispatched to the correct model based on its numeric index.  \n\nArguments\n\nunique_species::Vector{String}: List of species names.\nspecies_idx::Dict{String,Int}: Mapping from species name to numeric index.\nG1_number::Int: Number of neurons in the G1Layer.\nR_cutoff::Float32: Cutoff radius for the G1Layer.\n\nReturns\n\nspecies_models::Vector{Chain}: Array of models, ready for Enzyme differentiation.\n\nunique_species = [\"H\", \"O\"]\nspecies_idx = Dict(\"H\"=>1, \"O\"=>2)\nG1_number = 5\nR_cutoff = 5.0f0\n\nmodels = build_species_models(unique_species, species_idx, G1_number, R_cutoff; depth=2)\nprintln(models[1])  # model for H\nprintln(models[2])  # model for O\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.calculate_force-Tuple{AbstractVector, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.calculate_force","text":"calculate_force(model, x::AbstractVector) -> AbstractVector\n\nCompute the negative gradient of the scalar model output w.r.t. input x, i.e., the predicted forces.\n\nArguments\n\nmodel::Function: Callable model.\nx::AbstractVector: Single input structure.\n\nReturns\n\nAbstractVector: Predicted forces of same size as x.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.d_pbc-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}, AbstractMatrix{<:Real}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.d_pbc","text":"d_pbc(atom1, atom2, lattice; coords=:cartesian, return_image=false)\n\nCompute the minimum-image distance between two atoms under periodic boundary conditions (PBC).\n\nArguments\n\natom1::AbstractVector{<:Real}: Coordinates of the first atom.\natom2::AbstractVector{<:Real}: Coordinates of the second atom.\nlattice::AbstractMatrix{<:Real}: 3×3 lattice matrix where columns are lattice vectors.\ncoords::Symbol: Either :cartesian (default) or :fractional to specify the input coordinate type.\nreturn_image::Bool: If true, also return the minimum-image vector in Cartesian coordinates and the integer lattice translation vector applied.\n\nReturns\n\nd::Float32: Minimum distance under PBC.\nOptionally (d, rvec, n) if return_image=true:\nrvec::Vector{Float32}: Cartesian vector along the minimum-image direction.\nn::Vector{Int}: Integer lattice translation indices applied to obtain the minimum image.\n\nNotes\n\nWorks for both orthogonal and non-orthogonal lattices.\nImplements the minimum-image convention in fractional coordinates.\nSupports input coordinates in either Cartesian or fractional form.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.data_preprocess-Tuple{Any, Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.data_preprocess","text":"data_preprocess(input_data, energies, forces; split=[0.6, 0.2, 0.2])\n\nPreprocess input features and target data for neural network training. The dataset is split, energies normalized, forces rescaled consistently with energies, and targets repackaged into Sample structs.\n\nArguments\n\ninput_data::Array{<:Real,2}: Input features of shape (N_structures, 3 * n_atoms).\nenergies::Vector{Float32}: Total system energies for each structure.\nforces::Array{Float32,3}: Atomic forces of shape (N_structures, n_atoms, 3).\nsplit::Vector{Float64} (optional): Fractions for train/validation/test splits, must sum to 1. Default [0.6, 0.2, 0.2].\n\nReturns\n\nA tuple containing:\n\nx_train::Array, y_train::Vector{Sample}: Training inputs and targets.\nx_val::Array, y_val::Vector{Sample}: Validation inputs and targets.\nx_test::Array, y_test::Vector{Sample}: Test inputs and targets.\n(energy_mean::Float32, energy_std::Float32): Statistics used for energy normalization.\n\nNotes\n\nEnergies are normalized by Z-score (zero mean, unit variance).\nForces are scaled by the same energy standard deviation (energy_std).\nTargets are returned as Sample structs with:\n.energy: normalized scalar energy.\n.forces: rescaled force matrix (n_atoms, 3).\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.dispatch-Tuple{Any, Vector{Flux.Chain}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.dispatch","text":"dispatch(atoms, species_models::Vector{Chain}; lattice::Union{Nothing, Matrix{Float32}} = nothing)\n\nPublic API function.  The developer version is called dispatchtrain.  Computes the distance representation of a set of atoms (optionally within a lattice),   then applies the appropriate species-specific models via `dispatchtrain`.  \n\nArguments\n\natoms: Atomic structure input, suitable for distance_layer.  \nspecies_models::Vector{Chain}: One neural network model per species.  \nlattice::Union{Nothing, Matrix{Float32}}: Optional lattice matrix for periodic systems.    Defaults to nothing (no periodic boundary conditions).\n\nReturns\n\noutputs: Model predictions, either a scalar (single batch)    or a vector (batched input), depending on the input format.\n\n```julia\n\nSingle configuration\n\natoms = sampleatoms()  # returns a Vector{AtomInput} models = [modelH, model_O]  # one Flux.Chain per species result = dispatch(atoms, models)\n\nBatched configurations\n\nbatchatoms = [atoms1, atoms2, atoms3] batchresult = dispatch(batchatoms, models) println(batchresult)```\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.dispatch_train-Tuple{Matrix{SymmLearn.G1Input}, Vector{Flux.Chain}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.dispatch_train","text":"dispatch_train(distances::Matrix{G1Input}, species_models::Vector{Chain})\n\nApply the correct per-species neural network to a batch of atomic configurations.\n\nDescription\n\nThis is the batch version of dispatch_train.   Each row of distances represents a batch element, and each column represents an atom. The function applies the corresponding species model to each atom across all batches, then aggregates per-atom outputs into a scalar per batch.\n\nThis is a developer-level function; the public API is dispatch.\n\nArguments\n\ndistances::Matrix{G1Input}: Batched atomic inputs of size (n_batches, n_atoms).   Each entry contains species::Int and dist::Vector{Float32}.   Species is assumed consistent across batches for each atom.\nspecies_models::Vector{Chain}: One neural network model per species.\n\nReturns\n\nVector{Float32}: Vector of length n_batches, each element is the aggregated scalar prediction  (sum over atoms) for that batch.\n\nSee also\n\ndispatch_train(distances::Vector{G1Input}, species_models::Vector{Chain}) for a single-configuration version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.dispatch_train-Tuple{Vector{SymmLearn.G1Input}, Vector{Flux.Chain}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.dispatch_train","text":"dispatch_train(distances::Vector{G1Input}, species_models::Vector{Chain})\n\nApply the correct per-species neural network to a single atomic configuration.\n\nDescription\n\nThis is the single-configuration version of dispatch_train.   For each atom in the input vector, the function applies the corresponding neural network from species_models based on the atom's species field. The outputs are summed to produce a scalar prediction for the entire configuration.  \n\nThis is a developer-level function; the public API is dispatch.\n\nArguments\n\ndistances::Vector{G1Input}: Vector of atomic inputs for one configuration,   each element containing the atom's species ID and neighbor distances.\nspecies_models::Vector{Chain}: One neural network model per species, indexed by species.\n\nReturns\n\nFloat32: Aggregated scalar output for the configuration (sum of per-atom predictions).\n\nSee also\n\ndispatch_train(distances::Matrix{G1Input}, species_models::Vector{Chain}) for the batched version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.distance_derivatives-Tuple{Matrix{Vector{SymmLearn.AtomInput}}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.distance_derivatives","text":"distance_derivatives(input::Matrix{Vector{AtomInput}}; lattice=nothing)\n\nCompute analytical derivatives of pairwise interatomic distances for a batch of atomic systems.\n\nDescription\n\nThis is the batch version of distance_derivatives. Each row of input represents a separate  atomic configuration. For each configuration, the derivatives of distances between every pair of  atoms are computed. If lattice is provided, derivatives use the minimum-image convention under  periodic boundary conditions (PBC); otherwise, standard Cartesian derivatives are used.\n\nArguments\n\ninput::Matrix{Vector{AtomInput}}: 2D array of atomic systems. Each element is a vector of AtomInput objects containing .coord fields with 3D coordinates.\nlattice::Union{Nothing, Matrix{Float32}}: Optional 3×3 lattice matrix for PBC.\n\nReturns\n\nArray{Float32, 4}: Tensor of shape (n_batch, n_atoms, n_atoms-1, 3):\noutputs[b, i, j, :] → derivative of distance d(i, j+1) w.r.t atom i.\noutputs[b, j+1, i, :] → derivative of distance d(i, j+1) w.r.t atom j+1.\nDerivatives w.r.t. other atoms are zero (not stored).\n\nNotes\n\nIf two atoms coincide (distance numerically zero), the derivative is set to (0, 0, 0).\nSee also distance_derivatives(input::Vector{AtomInput}) for a single-configuration version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.distance_derivatives-Tuple{Vector{SymmLearn.AtomInput}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.distance_derivatives","text":"distance_derivatives(input::Vector{AtomInput}; lattice=nothing)\n\nCompute analytical derivatives of pairwise interatomic distances for a single atomic system.\n\nDescription\n\nThis is the single-configuration version of distance_derivatives. The function computes  derivatives of distances between every pair of atoms in the input vector. If lattice is provided,  the minimum-image convention under periodic boundary conditions (PBC) is applied; otherwise,  standard Cartesian derivatives are used.\n\nArguments\n\ninput::Vector{AtomInput}: Vector of atoms. Each AtomInput must have a .coord field with 3D coordinates.\nlattice::Union{Nothing, Matrix{Float32}}: Optional 3×3 lattice matrix for PBC.\n\nReturns\n\nArray{Float32, 3}: Tensor of shape (n_atoms, n_atoms-1, 3):\noutputs[i, j, :] → derivative of distance d(i, j+1) w.r.t atom i.\noutputs[j+1, i, :] → derivative of distance d(i, j+1) w.r.t atom j+1.\nDerivatives w.r.t. other atoms are zero (not stored).\n\nNotes\n\nIf two atoms coincide (distance numerically zero), the derivative is set to (0, 0, 0).\nSee also distance_derivatives(input::Matrix{Vector{AtomInput}}) for the batch version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.distance_layer-Tuple{Matrix{Vector{SymmLearn.AtomInput}}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.distance_layer","text":"distance_layer(input::Matrix{Vector{AtomInput}}; lattice=nothing)\n\nCompute pairwise distances for a batch of atomic configurations.\n\nDescription\n\nThis is the batch version of distance_layer, where each row of the matrix represents  a separate configuration of atoms. Distances are computed between every pair of atoms  in each configuration. If lattice is provided, the minimum-image convention under  periodic boundary conditions (PBC) is applied. Otherwise, simple Cartesian distances are used.\n\nArguments\n\ninput::Matrix{Vector{AtomInput}}: A matrix of batches. Each element is a vector of AtomInput objects containing .species::Int and .coord::AbstractVector (3D coordinates at least).\nlattice::Union{Nothing, Matrix{Float32}}: Optional 3×3 lattice matrix for PBC.\n\nReturns\n\nMatrix{G1Input}: Same shape as input. Each G1Input contains species and a 1×(N-1) matrix of distances.\n\nSee also\n\ndistance_layer(input::Vector{AtomInput}) for computing distances for a single atomic configuration.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.distance_layer-Tuple{Vector{SymmLearn.AtomInput}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.distance_layer","text":"distance_layer(input::Matrix{Vector{AtomInput}}; lattice=nothing)\n\nCompute pairwise distances for a batch of atomic configurations.\n\nDescription\n\nThis is the batch version of distance_layer, where each row of the matrix represents  a separate configuration of atoms. Distances are computed between every pair of atoms  in each configuration. If lattice is provided, the minimum-image convention under  periodic boundary conditions (PBC) is applied. Otherwise, simple Cartesian distances are used.\n\nArguments\n\ninput::Matrix{Vector{AtomInput}}: A matrix of batches. Each element is a vector of AtomInput objects containing .species::Int and .coord::AbstractVector (3D coordinates at least).\nlattice::Union{Nothing, Matrix{Float32}}: Optional 3×3 lattice matrix for PBC.\n\nReturns\n\nMatrix{G1Input}: Same shape as input. Each G1Input contains species and a 1×(N-1) matrix of distances.\n\nSee also\n\ndistance_layer(input::Vector{AtomInput}) for computing distances for a single atomic configuration.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.energy_loss-Tuple{Any, Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.energy_loss","text":"energy_loss(model, x, y) -> AbstractArray\n\nCompute the squared error between predicted energy and reference energy.\n\nArguments\n\nmodel::Function: A callable model.\nx: Input structure.\ny: Reference scalar energy.\n\nReturns\n\nSquared error as an array.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.extract_data-Tuple{String}","page":"SymmLearn.jl Developer API","title":"SymmLearn.extract_data","text":"extract_data(path::String)\n\nDescription:\n\nExtracts structural and energetic information from atomic configurations in .xyz format using ExtXYZ.\n\nArguments:\n\npath::String: Path to the file containing the data.\n\nReturns:\n\nA tuple containing:\n\natoms_in_a_cell::Int: Number of atoms in one cell (assumed constant across frames).\nspecies::Vector{String}: Species of atoms in the first frame.\nunique_species::Vector{String}: Unique species present in the system.\nall_cells::Vector{Matrix{Float32}}: List of cell matrices for each configuration.\ndataset::Array{Float32, 3}: 3D array with data per configuration:\nRows 1–3: Atomic positions (x, y, z).\nRows 4–6: Forces (fx, fy, fz).\nall_energies::Vector{Float32}: Energies for each configuration.\n\nFunctionality:\n\nReading the data: The function first reads the data from the provided path using read_frames(path).\nExtracting number of configurations: It calculates the total number of configurations by checking the size of the data.\nPre-allocating arrays: The function allocates memory for arrays to store the energies, cell matrices, atomic data, and forces for each configuration.\nSpecies extraction: It extracts the unique species (elements) used in the configurations and stores them in the species array.\nData extraction:\nThe function extracts the cell matrices for each configuration.\nIt also extracts the total energy for each configuration.\nThe dataset is populated with atomic charges, positions, and forces for each atom in each configuration.\nReturning the data: After all data has been extracted and stored in the arrays, the function returns the extracted data as a tuple.\n\nExample Usage:\n\natoms_in_a_cell, species,unique_species, all_cells, dataset, all_energies = extract_data(\"path/to/data.xyz\")\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.extract_energies-Tuple{SymmLearn.Sample}","page":"SymmLearn.jl Developer API","title":"SymmLearn.extract_energies","text":"extract_energies(x::Sample)\n\nReturn the energy of a single Sample.\n\nArguments\n\nx::Sample: a single sample containing energy and forces.\n\nReturns\n\nenergy::AbstractVector: the energy stored in the sample.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.extract_energies-Tuple{Vector{SymmLearn.Sample}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.extract_energies","text":"extract_energies(X::Vector{Sample})\n\nReturn the energies of a batch of samples.\n\nArguments\n\nX::Vector{Sample}: A collection of Sample objects, each containing an energy vector.\n\nReturns\n\nenergies_batch::Vector{Float32}: 1D vector of length n_batch, where each element contains the first component of the energy of a sample.\n\nNotes\n\nThe function extracts only the first element of each sample's energy field.\nOutput is always of type Float32.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.extract_forces-Tuple{SymmLearn.Sample}","page":"SymmLearn.jl Developer API","title":"SymmLearn.extract_forces","text":"extract_forces(x::Sample)\n\nReturn the forces of a single Sample.\n\nArguments\n\nx::Sample: a single sample containing energy and forces.\n\nReturns\n\nforces::AbstractVector: the forces stored in the sample.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.extract_forces-Tuple{Vector{SymmLearn.Sample}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.extract_forces","text":"extract_forces(y::Vector{Sample}; ndims::Int=3)\n\nExtract and reshape atomic force vectors from a batch of samples.\n\nArguments\n\ny::Vector{Sample}: A batch of Sample objects, each containing a forces field as a 2D array of shape (n_atoms, 3).\nndims::Int=3: Desired dimensionality of the returned array:\n3: Returns a 3D array (n_batch, n_atoms, 3)  \n2: Returns a 2D array (n_batch, n_atoms*3)  \n1: Returns a 1D array (n_batch*n_atoms*3)\n\nReturns\n\nArray of Float32 forces in the requested shape according to ndims.\n\nBehavior\n\nIf ndims == 3, output shape is (n_batch, n_atoms, 3).\nIf ndims == 2, output shape is (n_batch, n_atoms*3).\nIf ndims == 1, output shape is (n_batch*n_atoms*3).\nPrints an error message if ndims is not 1, 2, or 3.\n\nNotes\n\nAssumes all samples have the same number of atoms.\nForces are extracted directly from the forces field of each sample.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.fc-Tuple{Float32, Float32}","page":"SymmLearn.jl Developer API","title":"SymmLearn.fc","text":"fc(Rij, Rc)\n\nCompute a smooth cutoff function for distances between particles.\n\nArguments\n\nRij::Float32: Distance between two particles.\nRc::Float32: Cutoff distance. Returns 0 if Rij >= Rc.\n\nReturns\n\nFloat32: Value of the smooth cutoff function. \n\nBehavior\n\nReturns 0 if Rij >= Rc.\nFor Rij < Rc, computes a smooth exponential decay using:   fc(Rij) = exp(1 - 1 / (1 - (Rij / Rc)^2))\n\nA small tolerance (eps(Float32)) is used to avoid numerical issues when Rij approaches Rc.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.force_loss-Tuple{Any, AbstractVector, Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.force_loss","text":"force_loss(model, x::AbstractVector, f, f_matrix) -> Float32\n\nCompute the mean squared error (MSE) between predicted and reference forces for a single atomic structure.\n\nDescription\n\nThis is the single-structure version of force_loss.   It calculates the predicted forces by applying the model to the input representation x and mapping gradients via f_matrix, then compares them to the reference forces f using MSE.\n\nArguments\n\nmodel: Callable neural network model.\nx::AbstractVector: Input atomic structure or per-atom representation.\nf::AbstractMatrix{Float32}: Reference forces, shape (num_atoms, 3).\nf_matrix::AbstractArray{Float32, 3}: Distance derivative matrix for mapping gradients to Cartesian forces.\n\nReturns\n\nFloat32: Mean squared error between predicted and reference forces for the structure.\n\nSee also\n\nforce_loss(model, X::Matrix{G1Input}, F::Array{Float32,3}, F_matrix::Array{Float32,4}) for the batched version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.force_loss-Tuple{Any, Matrix{SymmLearn.G1Input}, Array{Float32, 3}, Array{Float32, 4}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.force_loss","text":"force_loss(model, X::Matrix{G1Input}, F::Array{Float32,3}, F_matrix::Array{Float32,4}) -> Vector{Float32}\n\nCompute force losses for a batch of atomic structures.\n\nDescription\n\nThis is the batch version of force_loss.   Each row of X corresponds to one structure. The function computes the predicted forces for each structure using the provided model and derivative matrices, then evaluates the mean squared error (MSE) against the reference forces.\n\nArguments\n\nmodel: Callable neural network model.\nX::Matrix{G1Input}: Batch of inputs, one row per structure.\nF::Array{Float32,3}: Reference forces for each structure, shape (num_samples, num_atoms, 3).\nF_matrix::Array{Float32,4}: Force derivative matrices for each structure, shape (num_samples, num_atoms, 3, 3).\n\nReturns\n\nVector{Float32}: Force loss for each structure in the batch.\n\nSee also\n\nforce_loss(model, x::AbstractVector, f, f_matrix) for the single-structure version.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.loss-Tuple{Any, Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.loss","text":"loss(model, x, y; λ=1.0f0, forces=true) -> Float32\n\nCompute the training loss as mean squared error on energies plus, optionally, a weighted force-matching term.\n\nThis is a user-level function; the developer version is loss_train.\n\nArguments\n\nmodel::Vector{Chain}: Species-specific neural network models.\nx: Input atomic structure(s).\ny: Reference labels containing energies and forces.\nλ::Float32 (optional): Weight for the force contribution. Default = 1.0.\nforces::Bool (optional): If true, include force loss. If false, only energy loss. Default = true.\n\nReturns\n\nFloat32: Total loss (energy-only if forces=false, energy + λ·forces otherwise).\n\n```julia\n\nAssume models, xbatch, ybatch are defined\n\ntotalloss = loss(models, xbatch, ybatch; λ=0.5f0, forces=true) println(\"Computed loss: \", totalloss)```\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.loss_train-NTuple{4, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.loss_train","text":"loss_train(models, x, y, fconst; λ=1.0f0) -> Float32\n\nCompute the combined energy and force loss for training.\n\nThis is a developer-level function; the public API is loss.\n\nArguments\n\nmodels: Callable model(s) for energy prediction.\nx: Input structure(s).\ny: Reference energies corresponding to x.\nfconst: Precomputed force contributions (treated as constant).\nλ::Float32 (optional): Weight for the force contribution. Default = 1.0.\n\nReturns\n\nFloat32: Total loss combining mean energy loss and weighted mean force term.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.maybe_decay_lr!-NTuple{7, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.maybe_decay_lr!","text":"maybe_decay_lr!(opt, current_lr, no_improve_count, patience, decay_factor, min_lr, epoch; verbose=false)\n\nCheck whether the learning rate should be decayed based on validation performance.\n\nIf the number of consecutive epochs without improvement (no_improve_count) reaches patience, the learning rate is multiplied by decay_factor, but not below min_lr. The optimizer state is updated in place to preserve momentum terms.\n\nArguments\n\nopt: Optimizer (e.g., Flux.Optimise.Adam) whose learning rate will be modified.\ncurrent_lr::Float32: Current learning rate.\nno_improve_count::Int: Number of consecutive epochs without improvement.\npatience::Int: Number of epochs to wait before decaying LR.\ndecay_factor::Float32: Factor to multiply learning rate when decaying.\nmin_lr::Float32: Minimum allowed learning rate.\nepoch::Int: Current epoch number.\nverbose::Bool=false: Print information when learning rate is decayed.\n\nReturns\n\nTuple (opt, current_lr, no_improve_count, stop_training::Bool):\n\nopt: Updated optimizer with new learning rate if decayed.\ncurrent_lr: Updated learning rate.\nno_improve_count: Reset to 0 if LR was decayed, else unchanged.\nstop_training::Bool: True if current_lr reached min_lr, signaling training should stop.\n\nNotes\n\nThis function allows adaptive learning rate schedules without restarting training.\nThe optimizer’s momentum buffers remain intact.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.maybe_save_best!-NTuple{7, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.maybe_save_best!","text":"maybe_save_best!(model, loss_val, epoch, best_model, best_loss, best_epoch, no_improve_count; tol=1.0)\n\nUpdate the record of the best-performing model based on validation loss.\n\nThis function checks whether the current validation loss loss_val is smaller than tol * best_loss.   If the condition is met, the current model is considered an improvement:\n\nbest_model is updated via deepcopy(model),\nbest_loss is set to loss_val,\nbest_epoch is updated to the current epoch,\nno_improve_count is reset to 0.\n\nIf there is no improvement, only no_improve_count is incremented by 1.\n\nArguments\n\nmodel: Current neural network model.\nloss_val::Float32: Validation loss for the current epoch.\nepoch::Int: Current epoch number.\nbest_model: Model corresponding to the best observed validation loss.\nbest_loss::Float32: Best validation loss recorded so far.\nbest_epoch::Int: Epoch number when best_model was saved.\nno_improve_count::Int: Number of consecutive epochs without improvement.\ntol::Float32=1.0: Tolerance factor. A new model is considered better if loss_val < tol * best_loss.\n\nReturns\n\nTuple (best_model, best_loss, best_epoch, no_improve_count) with updated values.\n\nNotes\n\nSetting tol < 1.0 allows small tolerance before updating the best model.\nThis function is useful for implementing early stopping and adaptive learning rate strategies in training loops.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.partition-Tuple{Vector, Vector{Float64}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.partition","text":"partition(data::Vector{<:AbstractArray}, parts::Vector{Float64}; shuffle=true, rng=Random.GLOBAL_RNG)\n\nSplit a list of datasets into multiple parts (e.g., train/val/test).\n\ndata: Vector of datasets (e.g., [x, e, f]), each can be array or vector of custom objects.\nparts: Vector of fractions summing to 1.0, e.g., [0.7, 0.2, 0.1].\nshuffle: Whether to shuffle indices before splitting.\nrng: Random number generator.\n\nReturns a tuple of vectors of splits for each dataset.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.predict_forces-Tuple{Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.predict_forces","text":"predict_forces(x, model; flat=false) -> Array{Float32}\n\nCompute predicted atomic forces for a batch of structures using a trained model.\n\nArguments\n\nx: Input atomic structures, either a Matrix{Vector{AtomInput}} (batched) or compatible with distance_layer.\nmodel: Species-specific neural network models used for force prediction.\nflat::Bool (optional): If true, return forces flattened as a 1D vector;   if false (default), return a 3D array (n_batches, n_atoms, 3).\n\nReturns\n\nArray{Float32}: Predicted forces for all atoms:\n(n_batches, n_atoms, 3) if flat=false\nFlattened 1D array if flat=true\n\nDescription\n\nCompute pairwise distances with distance_layer.\nCompute distance derivatives with distance_derivatives.\nFor each batch, compute force contributions from model gradients.\nOptionally flatten the resulting force array.\n\n```julia\n\nAssume x_test contains a batch of structures and models is defined\n\nforces = predictforces(xtest, models) println(size(forces))  # (nbatches, natoms, 3)\n\nforcesflat = predictforces(xtest, models; flat=true) println(length(forcesflat))  # nbatches * natoms * 3```\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.prepare_nn_data-Tuple{Array{Float32, 3}, Vector{String}, Vector{String}}","page":"SymmLearn.jl Developer API","title":"SymmLearn.prepare_nn_data","text":"prepare_nn_data(dataset::Array{Float32,3},\n                species_order::Vector{String},\n                unique_species::Vector{String})\n\nConvert a dataset of atomic positions and forces into atom-wise inputs for a neural network.\n\nArguments\n\ndataset: Array of shape (6, numatoms, numsamples).\nspecies_order: Vector of species strings, length = num_atoms, defining atom order.\nunique_species: Vector of unique species strings, used to build the mapping.\n\nReturns\n\nall_structures::Vector{Vector{AtomInput}}:   Each element is a structure represented as a vector of atoms.\nforces::Array{Float32,3}:   Shape (numsamples, numatoms, 3), atomic forces for each sample.\nspecies_idx::Dict{String,Int}:   Mapping from species name to integer index, consistent across atoms.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.train_model!-NTuple{5, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.train_model!","text":"train_model!(model, x_train, y_train, x_val, y_val, \n             λ=1.0f0; forces=true, initial_lr=0.01, min_lr=1e-6, decay_factor=0.1, \n             patience=50, epochs=1000, batch_size=32, verbose=false, lattice=nothing)\n\nTrain a neural network model to predict total energies (and optionally forces)  for atomic structures using mini-batch gradient descent, adaptive learning rate,  and early stopping.\n\nArguments\n\nmodel::Flux.Chain   Neural network to train. Can combine multiple branches for different atomic species.\nx_train::Any   Training atomic structures, either batched or compatible with distance_layer.\ny_train::Vector{Sample}   Ground-truth labels containing .energy and .forces for training.\nx_val::Any   Validation atomic structures, same format as x_train.\ny_val::Vector{Sample}   Ground-truth labels for validation.\nλ::Float32 (default = 1.0)   Weight applied to the force loss relative to the energy loss.\nforces::Bool (default = true)   If true, include force loss in addition to energy loss.\ninitial_lr::Float32 (default = 0.01)   Initial learning rate for the Adam optimizer.\nmin_lr::Float32 (default = 1e-6)   Minimum allowed learning rate; training stops decaying when reached.\ndecay_factor::Float32 (default = 0.1)   Factor by which to multiply the learning rate if validation loss plateaus.\npatience::Int (default = 50)   Number of epochs without improvement before reducing the learning rate.\nepochs::Int (default = 1000)   Maximum number of training epochs.\nbatch_size::Int (default = 32)   Number of structures per mini-batch.\nverbose::Bool (default = false)   If true, prints progress, learning rate changes, and final results.\nlattice::Union{Nothing, Matrix{Float32}} (default = nothing)   Optional 3×3 lattice matrix if distances should be computed under PBC.\n\nReturns\n\nbest_model::Flux.Chain   Model achieving the lowest validation loss during training.\nloss_tr::Vector{Float32}   Training loss per epoch.\nloss_val::Vector{Float32}   Validation loss per epoch.\n\nDescription\n\nPrecomputes pairwise distances (distance_layer) and derivatives (distance_derivatives) for both training and validation sets.\nPerforms mini-batch gradient descent using Enzyme.gradient and Flux.update!.\nOptionally computes forces in addition to energies in the loss function.\nApplies adaptive learning rate: decays learning rate if validation loss does not improve for patience epochs.\nSaves the model achieving the lowest validation loss (best_model).\nSupports early stopping if the learning rate reaches min_lr.\n\nNotes\n\nForces are computed using analytical derivatives of distances and backpropagated through the model.\nThe λ parameter balances energy and force contributions in the total loss.\nLoss values are monitored to prevent NaNs; training will stop if a NaN is detected.\n\n```julia\n\nAssume models, xtrain, ytrain, xval, yval are defined\n\nbestmodel, trainloss, valloss = trainmodel!(     model, xtrain, ytrain, xval, yval;     λ=0.5f0, forces=true, initiallr=0.01, epochs=500, batchsize=16, verbose=true )\n\nprintln(\"Training finished. Best validation loss: \", minimum(val_loss))```\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.update_lr!-Tuple{Any, Any}","page":"SymmLearn.jl Developer API","title":"SymmLearn.update_lr!","text":"update_lr!(opt, new_lr)\n\nRecursively update the learning rate eta for all Adam optimizer instances contained within a possibly nested optimizer structure.\n\nArguments\n\nopt: Optimizer or optimizer tree (e.g., OptimiserChain) potentially containing multiple Adam leaves.\nnew_lr::Float32: New learning rate to assign.\n\nReturns\n\nopt: The same optimizer object with updated learning rates.\n\nNotes\n\nOnly Adam optimizers (Flux.Optimise.Adam) are modified.\nOther optimizer types or wrappers are left unchanged.\nThis function modifies the optimizer in-place.\n\n\n\n\n\n","category":"method"},{"location":"developer/#SymmLearn.xyz_to_nn_input-Tuple{String}","page":"SymmLearn.jl Developer API","title":"SymmLearn.xyz_to_nn_input","text":"xyz_to_nn_input(file_path::String)\n\nProcess an XYZ file containing atomic structures and energies to generate datasets for neural network training.\n\nArguments\n\nfile_path::String: Path to the XYZ file containing coordinates, species, lattice cells, and energy values.\n\nReturns\n\nA tuple with:\n\nx_train::Array, y_train::Vector{Sample}: Training inputs and targets.\nx_val::Array, y_val::Vector{Sample}: Validation inputs and targets.\nx_test::Array, y_test::Vector{Sample}: Test inputs and targets.\n(energy_mean::Float32, energy_std::Float32): Energy normalization statistics.\nunique_species::Vector{String}: Unique atomic species in the dataset.\nspecies_idx::Dict{String,Int}: Mapping from species to integer indices.\nall_cells::Vector{Matrix{Float32}}: Lattice cell matrices for each configuration.\n\nDescription\n\nSteps performed:\n\nExtract structures, species, cells, and energies with extract_data.\nConvert positions and forces into atom-wise inputs with prepare_nn_data.\nNormalize energies and rescale forces consistently, then split into train/val/test sets with data_preprocess.\n\nDependencies\n\nextract_data(file_path): Parses raw atomic data from the XYZ file.  \nprepare_nn_data(dataset, species, unique_species): Builds NN inputs and forces arrays.  \ndata_preprocess(nn_input_dataset, all_energies, all_forces): Normalizes and splits the dataset.\n\nExample\n\nfile_path = \"example_structures.xyz\"\nx_train, y_train, x_val, y_val, x_test, y_test, (energy_mean, energy_std), unique_species, species_idx, all_cells = xyz_to_nn_input(file_path)\nprintln(\"Training set size: \", length(x_train))\nprintln(\"Unique species: \", unique_species)\n\n\n\n\n\n","category":"method"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"","category":"page"},{"location":"examples/#Predicting-the-Lennard-Jones-potential-betwenn-two-Helium-atoms","page":"Examples","title":"Predicting the Lennard-Jones potential betwenn two Helium atoms","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This is a simple example showing how to use this package to train a machine learning force field using your data.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In this example, we will study a system of two helium atoms interacting through a Lennard-Jones potential. The example is divided into two main sections:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Model Setup and Training:  We will load a pre-generated dataset containing configurations of two helium atoms in a simulation box and use the SymmLearn functions to build and train our neural network model.\nResults and Comparison:  Then, we will analyze the results. For this simple case, it is possible to visualize the trained force field and compare it to the reference Lennard-Jones potential.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Symmlearn\nusing Plots\nusing Random\n","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The loading process of the .xyz dataset con be done as illustrated here in the next code block.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Everything is fuly explained in the documentation, it's important to mention that the target data is stored in a Vector of Sample structs, containing both the sample energy and the forces","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This dataset consists in 1000 samples, for each the energy was computed using a Lennard-Jones potential with sigma = 1 and epsilon = 1.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"For each sample the distance between the two Helium atoms was randomly generated between 0.95 sigma and 2.5 sigma, one atom is in the origin the other is on the x axis","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"file_path = \"helium_LJ_dataset.xyz\"\n\nx_train, y_train, x_val, y_val, x_test, y_test, e_mean,e_std, unique_species, species_idx, _= \n    xyz_to_nn_input(file_path)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The xyztonn_input function returns the data already split in test, train and validation, the mean and the standard deviation of the energies  to renormalize them later and the lattice parameters, used by the model to compute the atomic distances with periodic boundary conditions ( in this example we won't be using PBC as the helium atoms are confined in a box )","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In the next block we will build the model and check how to compute all the useful objects","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"N_G1 = 5 #define the model using 10 G1 symmetry functions\nr_cutoff = 2.75f0\n\n#we set the random seed for reproducibility \n\nseed = 42\nRandom.seed!(seed)   \n\n#we use depth = 1, a branch with a small number of parameters, enough for this task\nmodel = build_species_models(unique_species, species_idx, N_G1, r_cutoff , depth = 1 , seed = seed) \n\n#We can check if the model and the loss work as we expected on a small batch \n\nx_batch = x_train[1:3, :]\ny_batch = y_train[1:3]","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now we can train the model","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"#we use both forces and energies to train the model  \n \n\n trained_model, train_loss, val_loss = train_model!(\n        model,\n        x_train, y_train,\n        x_val, y_val;\n         forces = false,  epochs = 500 , initial_lr = 1e-3 )\n","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Our model has been trained, we can look at the results","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"This first plot compares the energy of each pair as a function of the distance between the two atoms with the LJ potential for the test set","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Denormalize model predictions for the test set using Z-score inversion\ntest_guess = dispatch(x_test, trained_model) .* e_std .+ e_mean\n# Extract interatomic distances from the test set (for plotting)\nr_ij = [only(x_test[i][1].coord[1]) for i in 1:size(x_test, 1)]\n\n#We can also make the data for the reference plot\n\nf_lj(x) = 4*(1/x^(12) - 1/x^(6))\n\nx = 0.95:0.01:2.6\ny = f_lj.(x)\n\n\nplot(x, y,\n    label=\"Lennard-Jones Potential\",\n    color=\"black\",\n    lw=2\n)\n\n# Plot predictions vs Lennard-Jones potential\nscatter!(r_ij, test_guess,\n    label=\"Model prediction (Test Set)\",\n    alpha=0.5,\n    color=\"cyan\"\n)\n\n\n\nxlabel!(\"Distance [σ]\")\nylabel!(\"Energy [ϵ]\")\ntitle!(\"Helium energy prediction - Test Set\")","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: LJ energy prediction)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"As expected the model managed to reproduce very well the Lennard Jones potential","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"It's possible to do an analogous analysis to see how well the model predicts the forces","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# first we compute the forces using the model\n\nf_guess = predict_forces(x_test, trained_model)\n\n#then we extract the x axis forces applied to atom 1 and to atom 2\n#f_guess in an object with dimensions ( batch , atom , [F_x , F_y , F_z])\n\nf_guess_1 = f_guess[ : , 1 , 1] #we only care about the forces on the x axis\n\nf_guess_2 = f_guess[ : , 2 , 1]\n\n#this is the analytical formula for the forces applied to atom 1\n#the forces on atom 2 will just be -F(x)\n\nF(x) = 24 * (2/x^13 - 1/x^7)\n\nx = 0.95:0.01:2.6\ny_1 = F.(x)\ny_2 = - F.(x)\n\n\n\n# ---- Plot ----\n\n# Model predictions\nplot(x, y_1,\n    label=\"Real forces - Atom 1 \",\n    color=\"red\",\n    linestyle =:dash,\n    lw = 2\n)\n\nplot!(x, y_2,\n    label=\"Real forces - Atom 2 \",\n    color=\"blue\",\n    linestyle =:dash,\n    lw = 2\n)\n\n  scatter!(r_ij , f_guess_1,\n    label=\"Guess - Atom 1\",\n    color=\"orange\",\n    alpha = 0.5\n)\n\n  scatter!(r_ij , f_guess_2,\n    label=\"Guess - Atom 2\",\n    color=\"cyan\",\n    alpha = 0.5\n)\n\n\n# Labels\nylabel!(\"Force [ϵ ÷ σ]\")\nxlabel!(\"Distance between atoms [σ]\")\ntitle!(\"Force plot - Test Set\")\nylims!(minimum(y_2)*1.01 , maximum(y_1)*1.01)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: LJ forces prediction)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Also the forces are well reproduced by the model","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"A more quantitative analysis can be done by computing the rmse per atom for both the forces and the energies, that's what we will do in this next and last block","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"# ---- Energies ----\ne_test = extract_energies(y_test) .* e_std .+ e_mean\nn_atoms = 2\nrmse_total_e = sqrt(mean((test_guess .- e_test).^2))\nrmse_per_atom = rmse_total_e / n_atoms\n\n\np1 = scatter(test_guess , e_test,\n    label=\"Correlation plot\",\n    color=\"blue\",\n    alpha = 0.7,\n    xlabel = \"Predicted energy [ϵ]\",\n    ylabel = \"Real energy [ϵ]\",\n    title = \"Energy correlation plot\"\n)\nplot!(p1, e_test, e_test,\n    label=\"reference line\",\n    color=\"black\",\n    style=:dash\n)\n\ndisplay(p1)\n\n\n\n# ---- Forces ----\n#the flat = true option is useful only when making plots like this one\n#looking at the data in this form is not suggested\n\nf_test = extract_forces(y_test , ndims = 1 ) \n#always remember that to preserve the physical relation the forces are also normalized using e_std\nf_guess = predict_forces(x_test, trained_model ; flat = true  ) / e_std \nrmse_f = sqrt(mean((f_guess .- f_test).^2)) / n_atoms\nprintln(\"Force RMSE per atom = $(round(rmse_f, digits=4)) ϵ/σ\")\n\np2 = scatter(f_guess , f_test,\n    label=\"Correlation plot\",\n    color=\"blue\",\n    alpha = 0.7,\n    xlabel = \"Predicted force [ϵ/σ]\",\n    ylabel = \"Real force [ϵ/σ]\",\n    title = \"Force correlation plot\"\n)\nplot!(p2, f_test, f_test,\n    label=\"reference line\",\n    color=\"black\",\n    style=:dash\n)\n\ndisplay(p2)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: Energy correlation plot)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"(Image: Force correlation plot)","category":"page"},{"location":"examples/#Molecular-Dynamics-using-the-Molly-Interface-(TODO)","page":"Examples","title":"Molecular Dynamics using the Molly Interface (TODO)","text":"","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/","page":"Installation","title":"Installation","text":"SymmLearn.jl is available for Julia v1.10.2 and later releases, and can be installed with Julia built-in package manager.  In a Julia session, after entering the package manager mode with ], run the command","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"pkg> add Measurements","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"or else you can install the package directly from its source repository using Julia's package manager:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/yourusername/SymmLearn.jl\")","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"Alternatively, if you have cloned the repository locally, you can use:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"using Pkg\nPkg.develop(path=\"/path/to/SymmLearn\")","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"After installation, you can load the package with:","category":"page"},{"location":"installation/","page":"Installation","title":"Installation","text":"using SymmLearn","category":"page"},{"location":"#SymmLearn.jl","page":"Home","title":"SymmLearn.jl","text":"","category":"section"},{"location":"#What-Is-This-Package-Useful-For?","page":"Home","title":"What Is This Package Useful For?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SymmLearn.jl provides a framework to predict energies and forces of atomic systems using machine learning. It automates the generation of atom-centered representations via symmetry functions and the application of species-specific neural networks. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The framework is based on the Behler-Parrinello Neural Network approach: each neural network corresponds to a single atom type and predicts its contribution to the total energy. The total energy of the system is obtained by summing all atomic neural network contributions, and forces are computed by derivating the total energy respect to each atom coordinates.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Read more about Behler-Parrinello potentials here.","category":"page"},{"location":"#Citation-and-license-(TODO)","page":"Home","title":"Citation and license (TODO)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is written in Julia, a modern high-level, high-performance dynamic programming language designed for technical computing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The SymmLearn.jl package is licensed under the MIT \"Expat\" License. The original author is Gianmarco Biagi.","category":"page"}]
}
